{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This notebook is pat of an article not published, yet."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation: Fine-tunine\n",
    "The fine-tuning was created based on https://huggingface.co/docs/transformers/tasks/image_classification\n",
    "- Load dataset CIFAR10\n",
    "- Load google/vit-base-patch16-224-in21k\n",
    "- Fine-tune the model on CIFAR10\n",
    "- Store a checkpoint of the fine-tuned model for each frame of the animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Install required libraries\n",
    "!pip install transformers[torch] datasets  pandas pillow cleanlab scipy matplotlib imageio renumics-spotlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load cifar10 dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"cifar10\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the label2id and id2label dicts mapping labels to index values and vice versa.\n",
    "labels = ds[\"train\"].features[\"label\"].names\n",
    "label2id, id2label = dict(), dict()\n",
    "for i, label in enumerate(labels):\n",
    "    label2id[label] = str(i)\n",
    "    id2label[str(i)] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load an image preprocessor for the model. This will resize the images to the correct size for the model, and also apply any other transformations that are necessary.\n",
    "from transformers import AutoImageProcessor\n",
    "\n",
    "checkpoint = \"google/vit-base-patch16-224-in21k\"\n",
    "image_processor = AutoImageProcessor.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a list of PIL images and turn them to pixel values\n",
    "\n",
    "\n",
    "def transform(example_batch):\n",
    "    inputs = image_processor(\n",
    "        [x.convert(\"RGB\") for x in example_batch[\"img\"]], return_tensors=\"pt\"\n",
    "    )\n",
    "    inputs[\"label\"] = example_batch[\"label\"]\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# Apply transform on the dataset\n",
    "prepared_ds = ds.with_transform(transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a data collator to create batches\n",
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model with AutoModelForImageClassification. Specify number of labels and the label mappings.\n",
    "from transformers import AutoModelForImageClassification, TrainingArguments, Trainer\n",
    "\n",
    "model = AutoModelForImageClassification.from_pretrained(\n",
    "    checkpoint,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use an additional callback to write the loss values of time into a .csv file.\n",
    "from transformers import TrainerCallback\n",
    "\n",
    "\n",
    "class PrinterCallback(TrainerCallback):\n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        _ = logs.pop(\"total_flos\", None)\n",
    "        if state.is_local_process_zero:\n",
    "            if len(logs) == 3:  # skip last row\n",
    "                with open(\"log.csv\", \"a\") as f:\n",
    "                    f.write(\",\".join(map(str, logs.values())) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training paramter: Choose a low save_step interval for many frame for the video later\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "batch_size = 8\n",
    "# Defining training arguments (set push_to_hub to false if you don't want to upload it to HuggingFace's model hub)\n",
    "training_args = TrainingArguments(\n",
    "    f\"vit-base-patch16-224-in21k-ft-cifar10_highres_train\",\n",
    "    remove_unused_columns=False,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=100,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=20,  # the movie will be created by checkpoint save in this interval. Lower values increase the number of frames\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=0.04,  # use 0.04 for testing with a few frames. Use highe values for long movies\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=20,\n",
    "    load_best_model_at_end=False,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    push_to_hub=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Trainer object abd start the training\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=prepared_ds[\"train\"],\n",
    "    eval_dataset=prepared_ds[\"test\"],\n",
    "    tokenizer=image_processor,\n",
    "    callbacks=[PrinterCallback],\n",
    ")\n",
    "\n",
    "# Train and save results\n",
    "train_results = trainer.train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of functions for embeddings and outliers\n",
    "- Function to create an embedding\n",
    "- Function to extract Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define functions to create embeddings from an individual checkpoint\n",
    "# Play from https://renumics.com/next/docs/playbook/huggingface-embedding\n",
    "import datasets\n",
    "from transformers import AutoFeatureExtractor, AutoModel\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def extract_embeddings(model, feature_extractor, image_name=\"image\"):\n",
    "    \"\"\"Utility to compute embeddings.\"\"\"\n",
    "    device = model.device\n",
    "\n",
    "    def pp(batch):\n",
    "        images = batch[\"image\"]\n",
    "        inputs = feature_extractor(images=images, return_tensors=\"pt\").to(device)\n",
    "        embeddings = model(**inputs).last_hidden_state[:, 0].cpu()\n",
    "\n",
    "        return {\"embedding\": embeddings}\n",
    "\n",
    "    return pp\n",
    "\n",
    "\n",
    "def huggingface_embedding(\n",
    "    df,\n",
    "    image_name=\"image\",\n",
    "    inplace=False,\n",
    "    modelname=\"google/vit-base-patch16-224\",\n",
    "    batched=True,\n",
    "    batch_size=24,\n",
    "):\n",
    "    # initialize huggingface model\n",
    "    feature_extractor = AutoFeatureExtractor.from_pretrained(modelname)\n",
    "    model = AutoModel.from_pretrained(modelname, output_hidden_states=True)\n",
    "\n",
    "    # create huggingface dataset from df\n",
    "    dataset = datasets.Dataset.from_pandas(df).cast_column(image_name, datasets.Image())\n",
    "\n",
    "    # compute embedding\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    extract_fn = extract_embeddings(model.to(device), feature_extractor, image_name)\n",
    "    updated_dataset = dataset.map(extract_fn, batched=batched, batch_size=batch_size)\n",
    "\n",
    "    df_temp = updated_dataset.to_pandas()\n",
    "\n",
    "    if inplace:\n",
    "        df[\"embedding\"] = df_temp[\"embedding\"]\n",
    "        return\n",
    "\n",
    "    df_emb = pd.DataFrame()\n",
    "    df_emb[\"embedding\"] = df_temp[\"embedding\"]\n",
    "\n",
    "    return df_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-100 from Huggingface hub and convert it to Pandas dataframe\n",
    "import datasets\n",
    "\n",
    "ds = datasets.load_dataset(\"cifar10\", split=\"test\").prepare_for_task(\n",
    "    \"image-classification\"\n",
    ")\n",
    "df = ds.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load loss from csv file into loss_df\n",
    "import pandas as pd\n",
    "\n",
    "loss_df = pd.read_csv(\"log.csv\", names=[\"loss\", \"learning_rate\", \"epoch\"])\n",
    "loss_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to get all available checkpoints as sorted folders\n",
    "import os\n",
    "import datasets\n",
    "import time\n",
    "\n",
    "\n",
    "def get_sorted_checkpoint_folders():\n",
    "    # list all subfolders of 'renumics/vit-base-patch16-224-in21k-ft-cifar10' that have checkpoint in the name\n",
    "    checkpoint_folders = [\n",
    "        x\n",
    "        for x in os.listdir(\"vit-base-patch16-224-in21k-ft-cifar10_highres_train\")\n",
    "        if \"checkpoint\" in x\n",
    "    ]\n",
    "\n",
    "    # sort the list of folders\n",
    "    sorted_checkpoint_folders = sorted(\n",
    "        checkpoint_folders, key=lambda x: int(x.split(\"-\")[-1])\n",
    "    )\n",
    "    sorted_checkpoint_folders = [\n",
    "        \"vit-base-patch16-224-in21k-ft-cifar10_highres_train\" + \"/\" + x\n",
    "        for x in sorted_checkpoint_folders\n",
    "    ]\n",
    "    return sorted_checkpoint_folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings with each checkpont and store them in the same folder\n",
    "for sorted_checkpoint_folder in get_sorted_checkpoint_folders():\n",
    "    # check if embedding already exists\n",
    "    if os.path.exists(sorted_checkpoint_folder + \"/embedding.pkl\"):\n",
    "        continue\n",
    "    embedding = huggingface_embedding(\n",
    "        df, modelname=sorted_checkpoint_folder, image_name=\"image\"\n",
    "    )[\"embedding\"]\n",
    "    # store in smae folder\n",
    "    embedding.to_pickle(sorted_checkpoint_folder + \"/embedding.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to extract outliers based on embeddings using cleanlab\n",
    "import io\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from cleanlab.outlier import OutOfDistribution\n",
    "\n",
    "\n",
    "def get_ood(sorted_checkpoint_folder, df):\n",
    "    embedding = pd.read_pickle(sorted_checkpoint_folder + \"/embedding.pkl\").to_list()\n",
    "    embedding_np = np.array(embedding)\n",
    "\n",
    "    ood = OutOfDistribution()\n",
    "    ood_train_feature_scores = ood.fit_score(features=embedding_np)\n",
    "    df[\"scores\"] = ood_train_feature_scores\n",
    "\n",
    "    # select row with the lowest 8 scores\n",
    "    df_ood = df.sort_values(by=[\"scores\"], ascending=True).head(8)\n",
    "    # load the 8 corresponding images\n",
    "    df_ood_images = [\n",
    "        (Image.open(io.BytesIO(x[\"bytes\"])).convert(\"RGB\"), l)\n",
    "        for x, l in df_ood[[\"image\", \"labels\"]].to_numpy()\n",
    "    ]\n",
    "    return df_ood_images"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition of function for PCA and Procrustes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function to generate a PCA, use procrustes to transfor the created points to given input if provied.\n",
    "# this can be used to flip, rotate and scale the points of the new frame on the old frame to stabalize the movie.\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import procrustes\n",
    "\n",
    "\n",
    "def make_pca(sorted_checkpoint_folder, pca_np):\n",
    "    embedding = pd.read_pickle(sorted_checkpoint_folder + \"/embedding.pkl\").to_list()\n",
    "    embedding_np = np.array(embedding)\n",
    "    embedding_np_flat = embedding_np.reshape(-1, 768)\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_np_new = pca.fit_transform(embedding_np_flat)\n",
    "\n",
    "    if pca_np is None:\n",
    "        pca_np = pca_np_new\n",
    "\n",
    "    _, pca_np_new, disparity = procrustes(pca_np, pca_np_new)\n",
    "    pca_np = pca_np_new\n",
    "\n",
    "    # scale pca_np_new to be in range [-5, 5]\n",
    "    pca_np_disp = pca_np_new * 5 / np.max(np.abs(pca_np_new))\n",
    "    return pca_np_disp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review in Spotlight\n",
    "- load the first and latest checkpoint\n",
    "- generate embeddings, outliers and PCA\n",
    "- visualize in spotlight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load embeddings and extract outliers for first and last checkpoint\n",
    "first, last = get_sorted_checkpoint_folders()[0], get_sorted_checkpoint_folders()[-1]\n",
    "df_ood_images_first = get_ood(first, df)\n",
    "df[\"scores_first\"] = df[\"scores\"]\n",
    "df_ood_images_last = get_ood(last, df)\n",
    "df[\"scores_last\"] = df[\"scores\"]\n",
    "del df[\"scores\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply PCA to embeddings of first and last checkpoint and store them in the dataframe\n",
    "df[\"pca_first\"] = pca_np_disp_first = make_pca(first, None).tolist()\n",
    "df[\"pca_last\"] = pca_np_disp_last = make_pca(last, pca_np_disp_first).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add label_str column to dataframe\n",
    "df[\"label_str\"] = df[\"labels\"].apply(lambda x: ds.features[\"labels\"].int2str(x))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize embeddings and outliers of first and last checkpoint with spotlight\n",
    "from renumics import spotlight\n",
    "\n",
    "spotlight.show(\n",
    "    df,\n",
    "    dtype={\n",
    "        \"image\": spotlight.Image,\n",
    "        \"pca_first\": spotlight.Embedding,\n",
    "        \"pca_last\": spotlight.Embedding,\n",
    "    },\n",
    "    layout=\"https://spotlight.renumics.com/resources/embeddings_pca.json\",\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the animation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an image for each checkpoint and store it next to the checkpoint\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8), dpi=200)\n",
    "all_labels = ds.to_pandas()[\"labels\"]\n",
    "pca_np_disp = None\n",
    "for i, sorted_checkpoint_folder in tqdm.tqdm(\n",
    "    enumerate(get_sorted_checkpoint_folders())\n",
    "):\n",
    "    df_ood_images = get_ood(sorted_checkpoint_folder, df)\n",
    "    pca_np_disp = make_pca(sorted_checkpoint_folder, pca_np_disp)\n",
    "\n",
    "    # prepare figure\n",
    "    fig.clf()\n",
    "    a0, a1 = fig.subplots(2, 1, gridspec_kw={\"height_ratios\": [5, 1], \"hspace\": 0.4})\n",
    "    _ = fig.suptitle(\n",
    "        \"Fine Tuning Training Step \" + str(i * 2) + \" of a Vision Transformer (ViT)\"\n",
    "    )\n",
    "\n",
    "    # setup subplot of pca points\n",
    "    a0.set_aspect(\"equal\", adjustable=\"box\")\n",
    "    a0.set_xlim(-5, 5)\n",
    "    a0.set_ylim(-5, 5)\n",
    "    _ = a0.set_xlabel(\"pca 1\")\n",
    "    _ = a0.set_ylabel(\"pca 2\")\n",
    "    _ = a0.set_title(\"PCA of embedding space\")\n",
    "\n",
    "    # add a scatter plot one by one for each label\n",
    "    for k in range(10):\n",
    "        mask = all_labels == k\n",
    "        _ = a0.scatter(pca_np_disp[mask, 0], pca_np_disp[mask, 1])\n",
    "    a0.legend(\n",
    "        labels=[ds.features[\"labels\"].int2str(x) for x in range(10)], loc=\"upper right\"\n",
    "    )\n",
    "\n",
    "    # setup subplot for loss\n",
    "    _ = a1.set_ylim(0, 3)\n",
    "    _ = a1.set_xlim(0, max(loss_df[\"epoch\"]))\n",
    "    _ = a1.set_xlabel(\"step\")\n",
    "    _ = a1.set_ylabel(\"loss\")\n",
    "    _ = a1.set_title(\"Training loss\")\n",
    "\n",
    "    # plot loss\n",
    "    loss = loss_df[\"loss\"].copy()\n",
    "    if i + 1 < len(loss):\n",
    "        loss[i + 1 :] = np.nan\n",
    "    _ = a1.plot(loss_df[\"epoch\"], loss, c=\"r\")\n",
    "\n",
    "    # add oulier images\n",
    "    for j, (img, l) in enumerate(df_ood_images):\n",
    "        newax = fig.add_axes([0.85, 0.87 - (j / 11), 0.06, 0.07], anchor=\"NE\", zorder=1)\n",
    "        newax.imshow(img)\n",
    "        newax.axis(\"off\")\n",
    "        newax.set_aspect(\"equal\", \"box\")\n",
    "        newax.set_title(\"Outlier \" + str(j) + f\" ({ds.features['labels'].int2str(l)})\")\n",
    "\n",
    "    plt.savefig(sorted_checkpoint_folder + \"/pca_dyn_procrustes_300_outlow.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import re\n",
    "import imageio\n",
    "\n",
    "# get all images from candidates\n",
    "img_paths = []\n",
    "for sorted_checkpoint_folder in get_sorted_checkpoint_folders():\n",
    "    img_paths += glob.glob(\n",
    "        sorted_checkpoint_folder + \"/pca_dyn_procrustes_300_outlow.png\"\n",
    "    )\n",
    "# sort images by number\n",
    "img_paths = sorted(img_paths, key=lambda x: int(re.findall(r\"\\d+\", x)[0]))\n",
    "\n",
    "\n",
    "with imageio.get_writer(\n",
    "    \"pca_dyn_procrustes_300_outlow.gif\", mode=\"I\", loop=0\n",
    ") as writer:\n",
    "    for filename in img_paths[: 292 // 2]:\n",
    "        image = imageio.imread(filename)\n",
    "        # crop whitespace in image\n",
    "        image = image[10:-100, 110:-10]\n",
    "\n",
    "        writer.append_data(image)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checkout the article at tdb for more details."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
